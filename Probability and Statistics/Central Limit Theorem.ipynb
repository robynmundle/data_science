{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eece3038-104f-4903-b200-2dcf2a457967",
   "metadata": {},
   "source": [
    "# Central Limit Theorem\n",
    "The Central Limit Theorem, or CLT for short, is an important finding and pillar in the fields of statistics and probability. The theorem states that as the size of the sample increases, the distribution of the mean across multiple samples will approximate a Gaussian distribution. We can imagine performing a trial and getting a result or an observation. We can repeat the trial again and get a new independent observation. Collected together, multiple observations represents a sample of observations.\n",
    "\n",
    "A sample is a group of observations from a broader population of all possible observations that could be made given trials.\n",
    "* Observation: Result from one trial of an experiment.\n",
    "* Sample: Group of results gathered from separate independent trials.\n",
    "* Population: Space of all possible observations that could be seen from a trial.\n",
    "\n",
    "If we calculate the mean of a sample, it will be an estimate of the mean of the population distribution. But, like any estimate, it will be wrong and will contain some error. If we draw multiple independent samples, and calculate their means, the distribution of those means will form a Gaussian distribution. It is important that each trial that results in an observation be independent and performed in the same way. This is to ensure that the sample is drawing from the same underlying population distribution. More formally, this expectation is referred to as independent and identically distributed, or iid.\n",
    "* Firstly, the central limit theorem is impressive, especially as this will occur no matter the shape of the population distribution from which we are drawing samples. It demonstrates that the distribution of errors from estimating the population mean fit a distribution that the field of statistics knows a lot about.\n",
    "* Secondly, this estimate of the Gaussian distribution will be more accurate as the size of the samples drawn from the population is increased. This means that if we use our knowledge of the Gaussian distribution in general to start making inferences about the means of samples drawn from a population, that these inferences will become more useful as we increase our sample size.\n",
    "\n",
    "Online Resources:\n",
    "* https://machinelearningmastery.com/a-gentle-introduction-to-the-central-limit-theorem-for-machine-learning/\n",
    "\n",
    "# Law of Large Numbers\n",
    "The law of large numbers is another different theorem from statistics. It is simpler in that it states that as the size of a sample is increased, the more accurate of an estimate the sample mean will be of the population mean.\n",
    "\n",
    "This theorem describes the result of repeating the same experiment a large number of times. The large numbers theorem states that if the same experiment or study is repeated independently, a large number of times, the average of the results of the trials must be close to the expected value. The result becomes closer to the expected value as the number of trials increases.\n",
    "\n",
    "The law of large numbers is an important concept in statistics because it states that even random events with a large number of trials may return stable long-term results. It also demonstrates and proves the fundamental relationship between the concepts of probability and frequency.\n",
    "\n",
    "The central limit theorem does not state anything about a single sample mean; instead, it is broader and states something about the shape or the distribution of sample means. The law of large numbers is intuitive. It is why we think that collecting more data will lead to a more representative sample of observations from the domain. The theorem supports this intuition.\n",
    "\n",
    "# Hypothesis Test\n",
    "A hypothesis test is a statistical test that is used to determine whether there is enough evidence in a sample of data to infer that a certain condition is true for the entire population.Based on sample data, the test determines whether to reject the null hypothesis.\n",
    "* **Null hypothesis (H0)**: The null hypothesis states that a population parameter is equal to a certain value. The null hypothesis is often the initial claim that researchers specify using research or knowledge.\n",
    "* **Alternative Hypothesis (H1)**: The alternative hypothesis states that the population parameter is different from the value of the population parameter in the null hypothesis. The alternative hypothesis is what we might believe to be true or hope to prove true.\n",
    "* **p-Value**: The probability that random chance generated the data, or something else that is equal or rarer. We use the p-value, to make a determination: if the p-value is smaller than or equal to the level of significance, which is a cut-off point that we've defined, then we can reject the null hypothesis.\n",
    "    * Traditional threshold for significance is a p-value of 0.05 (95% CI) where a value below 0.05 is significant (typically noted as a rejection of the null hypothesis).\n",
    "* **Significance Level**: a threshold probability also known as the critical value ùõº. The area under the curve beyond the critical value is the critical region. The critical value defines how far away our sample statistic(our experimental value) must be from the null hypothesis(original mean) value before we can say it is unusual enough to reject the null hypothesis.\n",
    "* **Type 1 Error**: alike a false negative, if the significance level is 0.05, there is 5% change being wrong thus Type 1 Error = ùõº (significance level)\n",
    "* **Type 2 Error**: alike a false positive, concluding the null hypothesis is true or fail to reject when in reality it was actually false. Type 2 Errors are explained by ùõΩ\n",
    "* **Power**: probability of rejecting the null hypothesis when it is false. i.e. probability of committing type 2 error is 1 - (power) or 1-ùõΩ\n",
    "* **z-Test**: used when the difference in means is calculated between two distributions. Standard deviation of the population should be known and number of samples should be more than 30. Important assumption of x-test is that all sample observations are independent than each other\n",
    "* **T-Test**: used when the standard deviation of the population is unknown and has to be approximated from the sample. Generally used when two different populations are compared. Three types:\n",
    "    * Independent Samples: compares the means for two groups\n",
    "     * Paired Samples: compares mean from the same group at different times\n",
    "    * One Sample: tests mean of a single group against a known mean\n",
    "* **Chi-Square Test**: Generally used when testing is related to categorical variables. Two types:\n",
    "    * Goodness of Fit: test whether observations within one categorical variable match within a distribution\n",
    "    * Association: used to compare two variables in a contingency table to see if they are independent\n",
    "\n",
    "Online Resources:\n",
    "* https://towardsdatascience.com/hypothesis-testing-the-what-why-and-how-867d382b99ca\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
